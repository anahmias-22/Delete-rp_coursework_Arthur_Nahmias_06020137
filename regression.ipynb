{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1: DEFINITIONS AND HYPERPARAMETER SPACE ===\n",
      "\n",
      "=== STAGE 2: SPLIT DATASET INTO FIRST AND SECOND HALF ===\n",
      "Total dataset size: 499000\n",
      "First half: [0, 249500), Second half: [249500, 499000)\n",
      "\n",
      "=== STAGE 3: TRAINING MODELS ON THE FIRST HALF ===\n",
      "Loading features for (levels=5, window=10) from ./data/signature_features_depth_2_levels_5_window_10.npz\n",
      "100%|██████████| 20/20 [21:57<00:00, 65.87s/trial, best loss: 0.980104513720027]\n",
      "  >> TRAINED (levels=5, window=10, lag=50) -> Val R^2 = 0.0199, saved as ./models/lgb_levels_5_window_10_lag_50.joblib\n",
      "100%|██████████| 20/20 [24:51<00:00, 74.58s/trial, best loss: 0.9825436102464474] \n",
      "  >> TRAINED (levels=5, window=10, lag=55) -> Val R^2 = 0.0175, saved as ./models/lgb_levels_5_window_10_lag_55.joblib\n",
      "100%|██████████| 20/20 [25:35<00:00, 76.77s/trial, best loss: 0.9832702845751984] \n",
      "  >> TRAINED (levels=5, window=10, lag=60) -> Val R^2 = 0.0167, saved as ./models/lgb_levels_5_window_10_lag_60.joblib\n",
      "100%|██████████| 20/20 [23:04<00:00, 69.20s/trial, best loss: 0.9853087133659232] \n",
      "  >> TRAINED (levels=5, window=10, lag=65) -> Val R^2 = 0.0147, saved as ./models/lgb_levels_5_window_10_lag_65.joblib\n",
      "100%|██████████| 20/20 [1:07:50<00:00, 203.51s/trial, best loss: 0.9869007133704732]\n",
      "  >> TRAINED (levels=5, window=10, lag=70) -> Val R^2 = 0.0131, saved as ./models/lgb_levels_5_window_10_lag_70.joblib\n",
      "100%|██████████| 20/20 [25:55<00:00, 77.77s/trial, best loss: 0.9869371222521757] \n",
      "  >> TRAINED (levels=5, window=10, lag=75) -> Val R^2 = 0.0131, saved as ./models/lgb_levels_5_window_10_lag_75.joblib\n",
      "100%|██████████| 20/20 [25:04<00:00, 75.23s/trial, best loss: 0.9874870034910261] \n",
      "  >> TRAINED (levels=5, window=10, lag=80) -> Val R^2 = 0.0125, saved as ./models/lgb_levels_5_window_10_lag_80.joblib\n",
      "100%|██████████| 20/20 [24:40<00:00, 74.02s/trial, best loss: 0.9883392675752919] \n",
      "  >> TRAINED (levels=5, window=10, lag=85) -> Val R^2 = 0.0117, saved as ./models/lgb_levels_5_window_10_lag_85.joblib\n",
      "100%|██████████| 20/20 [25:28<00:00, 76.42s/trial, best loss: 0.9888739759691143] \n",
      "  >> TRAINED (levels=5, window=10, lag=87) -> Val R^2 = 0.0111, saved as ./models/lgb_levels_5_window_10_lag_87.joblib\n",
      "Loading features for (levels=5, window=20) from ./data/signature_features_depth_2_levels_5_window_20.npz\n",
      "100%|██████████| 20/20 [1:14:59<00:00, 224.95s/trial, best loss: 0.983960752382528]  \n",
      "  >> TRAINED (levels=5, window=20, lag=50) -> Val R^2 = 0.0160, saved as ./models/lgb_levels_5_window_20_lag_50.joblib\n",
      "100%|██████████| 20/20 [24:31<00:00, 73.57s/trial, best loss: 0.9853707526493314] \n",
      "  >> TRAINED (levels=5, window=20, lag=55) -> Val R^2 = 0.0146, saved as ./models/lgb_levels_5_window_20_lag_55.joblib\n",
      "100%|██████████| 20/20 [24:41<00:00, 74.06s/trial, best loss: 0.9871431338647687] \n",
      "  >> TRAINED (levels=5, window=20, lag=60) -> Val R^2 = 0.0129, saved as ./models/lgb_levels_5_window_20_lag_60.joblib\n",
      "100%|██████████| 20/20 [24:00<00:00, 72.04s/trial, best loss: 0.9889757000460541] \n",
      "  >> TRAINED (levels=5, window=20, lag=65) -> Val R^2 = 0.0110, saved as ./models/lgb_levels_5_window_20_lag_65.joblib\n",
      "100%|██████████| 20/20 [25:31<00:00, 76.58s/trial, best loss: 0.9909260788466657] \n",
      "  >> TRAINED (levels=5, window=20, lag=70) -> Val R^2 = 0.0091, saved as ./models/lgb_levels_5_window_20_lag_70.joblib\n",
      "100%|██████████| 20/20 [25:21<00:00, 76.06s/trial, best loss: 0.992242023080976] \n",
      "  >> TRAINED (levels=5, window=20, lag=75) -> Val R^2 = 0.0078, saved as ./models/lgb_levels_5_window_20_lag_75.joblib\n",
      "100%|██████████| 20/20 [24:02<00:00, 72.13s/trial, best loss: 0.9932230110915413] \n",
      "  >> TRAINED (levels=5, window=20, lag=80) -> Val R^2 = 0.0068, saved as ./models/lgb_levels_5_window_20_lag_80.joblib\n",
      "100%|██████████| 20/20 [21:16<00:00, 63.80s/trial, best loss: 0.9939290259325496]\n",
      "  >> TRAINED (levels=5, window=20, lag=85) -> Val R^2 = 0.0061, saved as ./models/lgb_levels_5_window_20_lag_85.joblib\n",
      "100%|██████████| 20/20 [21:35<00:00, 64.79s/trial, best loss: 0.9941876153418882]\n",
      "  >> TRAINED (levels=5, window=20, lag=87) -> Val R^2 = 0.0058, saved as ./models/lgb_levels_5_window_20_lag_87.joblib\n",
      "Loading features for (levels=5, window=50) from ./data/signature_features_depth_2_levels_5_window_50.npz\n",
      "100%|██████████| 20/20 [20:44<00:00, 62.21s/trial, best loss: 0.9946811744666137]\n",
      "  >> TRAINED (levels=5, window=50, lag=50) -> Val R^2 = 0.0053, saved as ./models/lgb_levels_5_window_50_lag_50.joblib\n",
      "100%|██████████| 20/20 [21:26<00:00, 64.31s/trial, best loss: 0.9959013683365698]\n",
      "  >> TRAINED (levels=5, window=50, lag=55) -> Val R^2 = 0.0041, saved as ./models/lgb_levels_5_window_50_lag_55.joblib\n",
      "100%|██████████| 20/20 [22:25<00:00, 67.27s/trial, best loss: 0.9959082701677547]\n",
      "  >> TRAINED (levels=5, window=50, lag=60) -> Val R^2 = 0.0041, saved as ./models/lgb_levels_5_window_50_lag_60.joblib\n",
      "100%|██████████| 20/20 [21:17<00:00, 63.89s/trial, best loss: 0.9966693831094937]\n",
      "  >> TRAINED (levels=5, window=50, lag=65) -> Val R^2 = 0.0033, saved as ./models/lgb_levels_5_window_50_lag_65.joblib\n",
      "100%|██████████| 20/20 [21:54<00:00, 65.73s/trial, best loss: 0.9974384972792737]\n",
      "  >> TRAINED (levels=5, window=50, lag=70) -> Val R^2 = 0.0026, saved as ./models/lgb_levels_5_window_50_lag_70.joblib\n",
      "100%|██████████| 20/20 [22:01<00:00, 66.05s/trial, best loss: 0.9973823459319344]\n",
      "  >> TRAINED (levels=5, window=50, lag=75) -> Val R^2 = 0.0026, saved as ./models/lgb_levels_5_window_50_lag_75.joblib\n",
      "100%|██████████| 20/20 [20:54<00:00, 62.74s/trial, best loss: 0.997673637512457] \n",
      "  >> TRAINED (levels=5, window=50, lag=80) -> Val R^2 = 0.0023, saved as ./models/lgb_levels_5_window_50_lag_80.joblib\n",
      "100%|██████████| 20/20 [22:11<00:00, 66.58s/trial, best loss: 0.9984133643220626]\n",
      "  >> TRAINED (levels=5, window=50, lag=85) -> Val R^2 = 0.0016, saved as ./models/lgb_levels_5_window_50_lag_85.joblib\n",
      "100%|██████████| 20/20 [22:03<00:00, 66.19s/trial, best loss: 0.9987471628766262]\n",
      "  >> TRAINED (levels=5, window=50, lag=87) -> Val R^2 = 0.0013, saved as ./models/lgb_levels_5_window_50_lag_87.joblib\n",
      "Loading features for (levels=5, window=100) from ./data/signature_features_depth_2_levels_5_window_100.npz\n",
      "100%|██████████| 20/20 [20:45<00:00, 62.29s/trial, best loss: 0.9989395079902367]\n",
      "  >> TRAINED (levels=5, window=100, lag=50) -> Val R^2 = 0.0011, saved as ./models/lgb_levels_5_window_100_lag_50.joblib\n",
      "100%|██████████| 20/20 [21:02<00:00, 63.13s/trial, best loss: 0.9990985459018967]\n",
      "  >> TRAINED (levels=5, window=100, lag=55) -> Val R^2 = 0.0009, saved as ./models/lgb_levels_5_window_100_lag_55.joblib\n",
      "100%|██████████| 20/20 [20:38<00:00, 61.94s/trial, best loss: 0.9994615631156577]\n",
      "  >> TRAINED (levels=5, window=100, lag=60) -> Val R^2 = 0.0005, saved as ./models/lgb_levels_5_window_100_lag_60.joblib\n",
      "100%|██████████| 20/20 [22:00<00:00, 66.00s/trial, best loss: 1.000087198799902] \n",
      "  >> TRAINED (levels=5, window=100, lag=65) -> Val R^2 = -0.0001, saved as ./models/lgb_levels_5_window_100_lag_65.joblib\n",
      "100%|██████████| 20/20 [21:07<00:00, 63.39s/trial, best loss: 1.000100690473642]\n",
      "  >> TRAINED (levels=5, window=100, lag=70) -> Val R^2 = -0.0001, saved as ./models/lgb_levels_5_window_100_lag_70.joblib\n",
      "100%|██████████| 20/20 [21:48<00:00, 65.43s/trial, best loss: 1.000227607477273]\n",
      "  >> TRAINED (levels=5, window=100, lag=75) -> Val R^2 = -0.0002, saved as ./models/lgb_levels_5_window_100_lag_75.joblib\n",
      "100%|██████████| 20/20 [21:48<00:00, 65.41s/trial, best loss: 1.0001524128030728]\n",
      "  >> TRAINED (levels=5, window=100, lag=80) -> Val R^2 = -0.0002, saved as ./models/lgb_levels_5_window_100_lag_80.joblib\n",
      "100%|██████████| 20/20 [20:11<00:00, 60.56s/trial, best loss: 1.0000674121148287]\n",
      "  >> TRAINED (levels=5, window=100, lag=85) -> Val R^2 = -0.0001, saved as ./models/lgb_levels_5_window_100_lag_85.joblib\n",
      "100%|██████████| 20/20 [20:35<00:00, 61.79s/trial, best loss: 0.9998578967269206]\n",
      "  >> TRAINED (levels=5, window=100, lag=87) -> Val R^2 = 0.0001, saved as ./models/lgb_levels_5_window_100_lag_87.joblib\n",
      "Loading features for (levels=5, window=200) from ./data/signature_features_depth_2_levels_5_window_200.npz\n",
      "100%|██████████| 20/20 [20:47<00:00, 62.39s/trial, best loss: 0.9996845533563375]\n",
      "  >> TRAINED (levels=5, window=200, lag=50) -> Val R^2 = 0.0003, saved as ./models/lgb_levels_5_window_200_lag_50.joblib\n",
      "100%|██████████| 20/20 [20:29<00:00, 61.46s/trial, best loss: 0.9998720399119414]\n",
      "  >> TRAINED (levels=5, window=200, lag=55) -> Val R^2 = 0.0001, saved as ./models/lgb_levels_5_window_200_lag_55.joblib\n",
      "100%|██████████| 20/20 [20:40<00:00, 62.01s/trial, best loss: 0.999639551334326]\n",
      "  >> TRAINED (levels=5, window=200, lag=60) -> Val R^2 = 0.0004, saved as ./models/lgb_levels_5_window_200_lag_60.joblib\n",
      "100%|██████████| 20/20 [20:25<00:00, 61.28s/trial, best loss: 0.9996348697479622]\n",
      "  >> TRAINED (levels=5, window=200, lag=65) -> Val R^2 = 0.0004, saved as ./models/lgb_levels_5_window_200_lag_65.joblib\n",
      "100%|██████████| 20/20 [20:02<00:00, 60.12s/trial, best loss: 0.9994052054371427]\n",
      "  >> TRAINED (levels=5, window=200, lag=70) -> Val R^2 = 0.0006, saved as ./models/lgb_levels_5_window_200_lag_70.joblib\n",
      "100%|██████████| 20/20 [21:24<00:00, 64.22s/trial, best loss: 0.9995615382189733]\n",
      "  >> TRAINED (levels=5, window=200, lag=75) -> Val R^2 = 0.0004, saved as ./models/lgb_levels_5_window_200_lag_75.joblib\n",
      "100%|██████████| 20/20 [20:25<00:00, 61.29s/trial, best loss: 0.999772460653493]\n",
      "  >> TRAINED (levels=5, window=200, lag=80) -> Val R^2 = 0.0002, saved as ./models/lgb_levels_5_window_200_lag_80.joblib\n",
      "100%|██████████| 20/20 [20:47<00:00, 62.37s/trial, best loss: 0.9997358185296975]\n",
      "  >> TRAINED (levels=5, window=200, lag=85) -> Val R^2 = 0.0003, saved as ./models/lgb_levels_5_window_200_lag_85.joblib\n",
      " 90%|█████████ | 18/20 [19:36<02:10, 65.38s/trial, best loss: 0.9998512641786425]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 149\u001b[0m\n\u001b[1;32m    146\u001b[0m y_lag_first_half \u001b[38;5;241m=\u001b[39m y_lag[:half_idx]\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Train + Tune\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m model_lag, r2_val, best_params \u001b[38;5;241m=\u001b[39m tune_and_train(\n\u001b[1;32m    150\u001b[0m     X_sub_first_half_scaled, \n\u001b[1;32m    151\u001b[0m     y_lag_first_half,\n\u001b[1;32m    152\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    155\u001b[0m trained_models[(levels, window, lag)] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model_lag,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_r2\u001b[39m\u001b[38;5;124m'\u001b[39m: r2_val,\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: best_params,\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m: scaler\n\u001b[1;32m    160\u001b[0m }\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Optionally save to disk\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mtune_and_train\u001b[0;34m(X, y, seed)\u001b[0m\n\u001b[1;32m     77\u001b[0m X_tr, X_val, y_tr, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[0;32m---> 80\u001b[0m best \u001b[38;5;241m=\u001b[39m fmin(\n\u001b[1;32m     81\u001b[0m     fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m sp: objective_func(sp, X_tr, y_tr, X_val, y_val, seed),\n\u001b[1;32m     82\u001b[0m     space\u001b[38;5;241m=\u001b[39mparam_space,\n\u001b[1;32m     83\u001b[0m     algo\u001b[38;5;241m=\u001b[39mtpe\u001b[38;5;241m.\u001b[39msuggest,\n\u001b[1;32m     84\u001b[0m     max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \n\u001b[1;32m     85\u001b[0m     trials\u001b[38;5;241m=\u001b[39mtrials,\n\u001b[1;32m     86\u001b[0m     rstate\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed)\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Convert discrete choices\u001b[39;00m\n\u001b[1;32m     89\u001b[0m best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboosting_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trials\u001b[38;5;241m.\u001b[39mfmin(\n\u001b[1;32m    541\u001b[0m         fn,\n\u001b[1;32m    542\u001b[0m         space,\n\u001b[1;32m    543\u001b[0m         algo\u001b[38;5;241m=\u001b[39malgo,\n\u001b[1;32m    544\u001b[0m         max_evals\u001b[38;5;241m=\u001b[39mmax_evals,\n\u001b[1;32m    545\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    546\u001b[0m         loss_threshold\u001b[38;5;241m=\u001b[39mloss_threshold,\n\u001b[1;32m    547\u001b[0m         max_queue_len\u001b[38;5;241m=\u001b[39mmax_queue_len,\n\u001b[1;32m    548\u001b[0m         rstate\u001b[38;5;241m=\u001b[39mrstate,\n\u001b[1;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[38;5;241m=\u001b[39mpass_expr_memo_ctrl,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    551\u001b[0m         catch_eval_exceptions\u001b[38;5;241m=\u001b[39mcatch_eval_exceptions,\n\u001b[1;32m    552\u001b[0m         return_argmin\u001b[38;5;241m=\u001b[39mreturn_argmin,\n\u001b[1;32m    553\u001b[0m         show_progressbar\u001b[38;5;241m=\u001b[39mshow_progressbar,\n\u001b[1;32m    554\u001b[0m         early_stop_fn\u001b[38;5;241m=\u001b[39mearly_stop_fn,\n\u001b[1;32m    555\u001b[0m         trials_save_file\u001b[38;5;241m=\u001b[39mtrials_save_file,\n\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fmin(\n\u001b[1;32m    672\u001b[0m     fn,\n\u001b[1;32m    673\u001b[0m     space,\n\u001b[1;32m    674\u001b[0m     algo\u001b[38;5;241m=\u001b[39malgo,\n\u001b[1;32m    675\u001b[0m     max_evals\u001b[38;5;241m=\u001b[39mmax_evals,\n\u001b[1;32m    676\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    677\u001b[0m     loss_threshold\u001b[38;5;241m=\u001b[39mloss_threshold,\n\u001b[1;32m    678\u001b[0m     trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    679\u001b[0m     rstate\u001b[38;5;241m=\u001b[39mrstate,\n\u001b[1;32m    680\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    681\u001b[0m     max_queue_len\u001b[38;5;241m=\u001b[39mmax_queue_len,\n\u001b[1;32m    682\u001b[0m     allow_trials_fmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# -- prevent recursion\u001b[39;00m\n\u001b[1;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[38;5;241m=\u001b[39mpass_expr_memo_ctrl,\n\u001b[1;32m    684\u001b[0m     catch_eval_exceptions\u001b[38;5;241m=\u001b[39mcatch_eval_exceptions,\n\u001b[1;32m    685\u001b[0m     return_argmin\u001b[38;5;241m=\u001b[39mreturn_argmin,\n\u001b[1;32m    686\u001b[0m     show_progressbar\u001b[38;5;241m=\u001b[39mshow_progressbar,\n\u001b[1;32m    687\u001b[0m     early_stop_fn\u001b[38;5;241m=\u001b[39mearly_stop_fn,\n\u001b[1;32m    688\u001b[0m     trials_save_file\u001b[38;5;241m=\u001b[39mtrials_save_file,\n\u001b[1;32m    689\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[38;5;241m.\u001b[39mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_evals \u001b[38;5;241m-\u001b[39m n_done, block_until_done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[8], line 81\u001b[0m, in \u001b[0;36mtune_and_train.<locals>.<lambda>\u001b[0;34m(sp)\u001b[0m\n\u001b[1;32m     77\u001b[0m X_tr, X_val, y_tr, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[1;32m     80\u001b[0m best \u001b[38;5;241m=\u001b[39m fmin(\n\u001b[0;32m---> 81\u001b[0m     fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m sp: objective_func(sp, X_tr, y_tr, X_val, y_val, seed),\n\u001b[1;32m     82\u001b[0m     space\u001b[38;5;241m=\u001b[39mparam_space,\n\u001b[1;32m     83\u001b[0m     algo\u001b[38;5;241m=\u001b[39mtpe\u001b[38;5;241m.\u001b[39msuggest,\n\u001b[1;32m     84\u001b[0m     max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \n\u001b[1;32m     85\u001b[0m     trials\u001b[38;5;241m=\u001b[39mtrials,\n\u001b[1;32m     86\u001b[0m     rstate\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed)\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Convert discrete choices\u001b[39;00m\n\u001b[1;32m     89\u001b[0m best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboosting_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 70\u001b[0m, in \u001b[0;36mobjective_func\u001b[0;34m(space, X_train, y_train, X_val, y_val, seed)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_func\u001b[39m(space, X_train, y_train, X_val, y_val, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Hyperopt objective. We want to maximize R^2 => minimize (1 - R^2).\"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     _, r2_val \u001b[38;5;241m=\u001b[39m train_lgb_model(space, X_train, y_train, X_val, y_val, seed)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m r2_val, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m, in \u001b[0;36mtrain_lgb_model\u001b[0;34m(params, X_train, y_train, X_val, y_val, seed)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train LightGBM and return (model, R^2) on validation.\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\n\u001b[1;32m     43\u001b[0m     boosting_type\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboosting_type\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     44\u001b[0m     colsample_bytree\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     60\u001b[0m     X_train, y_train,\n\u001b[1;32m     61\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)],\n\u001b[1;32m     62\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     65\u001b[0m r2_val \u001b[38;5;241m=\u001b[39m r2_score(y_val, y_val_pred)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/lightgbm/sklearn.py:1189\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1174\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m   1190\u001b[0m         X,\n\u001b[1;32m   1191\u001b[0m         y,\n\u001b[1;32m   1192\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1193\u001b[0m         init_score\u001b[38;5;241m=\u001b[39minit_score,\n\u001b[1;32m   1194\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39meval_set,\n\u001b[1;32m   1195\u001b[0m         eval_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[1;32m   1196\u001b[0m         eval_sample_weight\u001b[38;5;241m=\u001b[39meval_sample_weight,\n\u001b[1;32m   1197\u001b[0m         eval_init_score\u001b[38;5;241m=\u001b[39meval_init_score,\n\u001b[1;32m   1198\u001b[0m         eval_metric\u001b[38;5;241m=\u001b[39meval_metric,\n\u001b[1;32m   1199\u001b[0m         feature_name\u001b[38;5;241m=\u001b[39mfeature_name,\n\u001b[1;32m   1200\u001b[0m         categorical_feature\u001b[38;5;241m=\u001b[39mcategorical_feature,\n\u001b[1;32m   1201\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m   1202\u001b[0m         init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[1;32m   1203\u001b[0m     )\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/lightgbm/sklearn.py:955\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    952\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    953\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m    956\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    957\u001b[0m     train_set\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[1;32m    958\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators,\n\u001b[1;32m    959\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39mvalid_sets,\n\u001b[1;32m    960\u001b[0m     valid_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[1;32m    961\u001b[0m     feval\u001b[38;5;241m=\u001b[39meval_metrics_callable,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[1;32m    963\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    964\u001b[0m )\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/lightgbm/engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    296\u001b[0m     cb(\n\u001b[1;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m booster\u001b[38;5;241m.\u001b[39mupdate(fobj\u001b[38;5;241m=\u001b[39mfobj)\n\u001b[1;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xty_new/lib/python3.12/site-packages/lightgbm/basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4135\u001b[0m _safe_call(\n\u001b[0;32m-> 4136\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   4137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[1;32m   4138\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(is_finished),\n\u001b[1;32m   4139\u001b[0m     )\n\u001b[1;32m   4140\u001b[0m )\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK \n",
    "# hyperopt is a better gridsearch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###############################################################################\n",
    "# STAGE 1: DEFINITIONS AND HYPERPARAMETER SPACE\n",
    "###############################################################################\n",
    "print(\"=== STAGE 1: DEFINITIONS AND HYPERPARAMETER SPACE ===\")\n",
    "\n",
    "levels_list = [5, 10, 15]\n",
    "window_list = [10, 20, 50, 100, 200]\n",
    "lag_list = [50, 55, 60, 65, 70, 75, 80, 85, 87]  # We'll train on these lags\n",
    "MAIN_LAG = 87  # We evaluate final performance wrt y_87\n",
    "\n",
    "# LightGBM hyperparameter search space\n",
    "param_space = {\n",
    "    'boosting_type':  hp.choice('boosting_type', ['dart']),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 0.6),\n",
    "    'drop_rate':      hp.uniform('drop_rate', 0.4, 0.9),\n",
    "    'learning_rate':  hp.loguniform('learning_rate', np.log(0.001), np.log(0.1)),\n",
    "    'max_depth':      hp.choice('max_depth', [3, 4, 5, 6]),\n",
    "    'min_child_samples': hp.randint('min_child_samples', 500, 30001),\n",
    "    'min_child_weight':  hp.uniform('min_child_weight', 0.01, 0.05),\n",
    "    'n_estimators':   hp.choice('n_estimators', [500, 1000, 1500]),\n",
    "    'objective':      hp.choice('objective', ['regression']),\n",
    "    'skip_drop':      hp.uniform('skip_drop', 0.5, 0.9),\n",
    "    'subsample':      hp.uniform('subsample', 0.5, 1.0),\n",
    "}\n",
    "\n",
    "def train_lgb_model(params, X_train, y_train, X_val, y_val, seed=42):\n",
    "    \"\"\"Train LightGBM and return (model, R^2) on validation.\"\"\"\n",
    "    model = lgb.LGBMRegressor(\n",
    "        boosting_type=params['boosting_type'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        drop_rate=params['drop_rate'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_child_samples=params['min_child_samples'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        objective=params['objective'],\n",
    "        skip_drop=params['skip_drop'],\n",
    "        subsample=params['subsample'],\n",
    "        seed=seed,\n",
    "        verbosity=-1,\n",
    "        early_stopping_rounds=50, \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='r2'\n",
    "    )\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    r2_val = r2_score(y_val, y_val_pred)\n",
    "    return model, r2_val\n",
    "\n",
    "def objective_func(space, X_train, y_train, X_val, y_val, seed=42):\n",
    "    \"\"\"Hyperopt objective. We want to maximize R^2 => minimize (1 - R^2).\"\"\"\n",
    "    _, r2_val = train_lgb_model(space, X_train, y_train, X_val, y_val, seed)\n",
    "    return {'loss': 1.0 - r2_val, 'status': STATUS_OK}\n",
    "\n",
    "def tune_and_train(X, y, seed=42):\n",
    "    \"\"\"\n",
    "    Splits X,y into train/val, runs Hyperopt, returns final model & R^2.\n",
    "    \"\"\"\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=lambda sp: objective_func(sp, X_tr, y_tr, X_val, y_val, seed),\n",
    "        space=param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20, \n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(seed)\n",
    "    )\n",
    "    # Convert discrete choices\n",
    "    best['boosting_type'] = 'dart'\n",
    "    best['objective'] = 'regression'\n",
    "    max_depth_values = [3, 4, 5, 6]\n",
    "    n_estimators_values = [500, 1000, 1500]\n",
    "    best['max_depth'] = max_depth_values[best['max_depth']]\n",
    "    best['n_estimators'] = n_estimators_values[best['n_estimators']]\n",
    "\n",
    "    final_model, final_r2 = train_lgb_model(best, X_tr, y_tr, X_val, y_val, seed)\n",
    "    return final_model, final_r2, best\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# STAGE 2: SPLIT DATASET INTO FIRST HALF AND SECOND HALF\n",
    "###############################################################################\n",
    "print(\"\\n=== STAGE 2: SPLIT DATASET INTO FIRST AND SECOND HALF ===\")\n",
    "\n",
    "# We'll get N from one sample file\n",
    "sample_file = f'./data/signature_features_depth_2_levels_{levels_list[0]}_window_{window_list[0]}.npz'\n",
    "with np.load(sample_file) as data:\n",
    "    N = data['arr_0'].shape[0]\n",
    "\n",
    "half_idx = N // 2\n",
    "\n",
    "print(f\"Total dataset size: {N}\")\n",
    "print(f\"First half: [0, {half_idx}), Second half: [{half_idx}, {N})\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# STAGE 3: TRAIN ONE MODEL PER (levels, window, lag) ON THE FIRST HALF\n",
    "###############################################################################\n",
    "print(\"\\n=== STAGE 3: TRAINING MODELS ON THE FIRST HALF ===\")\n",
    "\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "trained_models = {}\n",
    "\n",
    "for levels in levels_list:\n",
    "    for window in window_list:\n",
    "        # Load features for this subset\n",
    "        feat_path = f'./data/signature_features_depth_2_levels_{levels}_window_{window}.npz'\n",
    "        print(f\"Loading features for (levels={levels}, window={window}) from {feat_path}\")\n",
    "        with np.load(feat_path) as fdata:\n",
    "            X_sub = fdata['arr_0']  # shape: (N, D_sub)\n",
    "        \n",
    "        # Split & scale\n",
    "        X_sub_first_half = X_sub[:half_idx]\n",
    "        scaler = StandardScaler()\n",
    "        X_sub_first_half_scaled = scaler.fit_transform(X_sub_first_half)\n",
    "        \n",
    "        for lag in lag_list:\n",
    "            label_path = f'./data/targets_lag_{lag}.npz'\n",
    "            with np.load(label_path) as ldata:\n",
    "                y_lag = ldata['targets']  # shape: (N,)\n",
    "            \n",
    "            y_lag_first_half = y_lag[:half_idx]\n",
    "            \n",
    "            # Train + Tune\n",
    "            model_lag, r2_val, best_params = tune_and_train(\n",
    "                X_sub_first_half_scaled, \n",
    "                y_lag_first_half,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            trained_models[(levels, window, lag)] = {\n",
    "                'model': model_lag,\n",
    "                'val_r2': r2_val,\n",
    "                'params': best_params,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "            \n",
    "            # Optionally save to disk\n",
    "            model_filename = f'./models/lgb_levels_{levels}_window_{window}_lag_{lag}.joblib'\n",
    "            joblib.dump(model_lag, model_filename)\n",
    "            print(f\"  >> TRAINED (levels={levels}, window={window}, lag={lag}) -> \"\n",
    "                  f\"Val R^2 = {r2_val:.4f}, saved as {model_filename}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# STAGE 4: DEFINE BLENDING AND TEST PORTIONS IN THE SECOND HALF (WITH A SKIP IN-BETWEEN)\n",
    "###############################################################################\n",
    "print(\"\\n=== STAGE 4: DEFINE BLENDING AND TEST PORTIONS IN SECOND HALF ===\")\n",
    "\n",
    "second_half_len = N - half_idx\n",
    "blend_size = second_half_len // 2  # e.g. half for blending, half for test\n",
    "blend_start = half_idx\n",
    "blend_end   = blend_start + blend_size\n",
    "\n",
    "skip_after_blend = 500  # SKIP AFTER THE BLENDING\n",
    "test_start = blend_end + skip_after_blend\n",
    "test_end   = N\n",
    "\n",
    "if test_start >= test_end:\n",
    "    raise ValueError(\"skip_after_blend is too large; no data left for testing.\")\n",
    "\n",
    "print(f\"Second half length = {second_half_len}\")\n",
    "print(f\"Blending portion: [{blend_start}, {blend_end}) => size {blend_end - blend_start}\")\n",
    "print(f\"Skipping {skip_after_blend} after blending => test portion: [{test_start}, {test_end}) => size {test_end - test_start}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# STAGE 5: LOAD y_87 (MAIN TARGET) FOR BLENDING AND TEST\n",
    "###############################################################################\n",
    "print(\"\\n=== STAGE 5: LOAD MAIN TARGET (y_87) FOR BLENDING & TEST ===\")\n",
    "\n",
    "label_87_path = f'./data/targets_lag_{MAIN_LAG}.npz'\n",
    "print(f\"Loading y_87 from {label_87_path}\")\n",
    "with np.load(label_87_path) as lbl87_data:\n",
    "    y_87 = lbl87_data['targets']  # shape: (N,)\n",
    "\n",
    "y_blend = y_87[blend_start:blend_end]\n",
    "y_test  = y_87[test_start:test_end]\n",
    "print(f\"  >> y_blend size = {y_blend.shape[0]}, y_test size = {y_test.shape[0]}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# STAGE 6: AVERAGE ACROSS LAGS (PER SUBSET), THEN BLEND SUBSETS USING ELASTICNET\n",
    "###############################################################################\n",
    "print(\"\\n=== STAGE 6: AVERAGE PREDICTIONS ACROSS LAGS, THEN BLEND SUBSETS ===\")\n",
    "\n",
    "subset_keys = list(set((lvl, w) for (lvl, w, _) in trained_models.keys()))\n",
    "subset_keys.sort()\n",
    "\n",
    "print(f\"Found {len(subset_keys)} subsets: {subset_keys}\")\n",
    "\n",
    "# 6a. Blending portion: get subset predictions\n",
    "blend_subset_preds = []\n",
    "for (levels, window) in subset_keys:\n",
    "    feat_path = f'./data/signature_features_depth_2_levels_{levels}_window_{window}.npz'\n",
    "    with np.load(feat_path) as fdata:\n",
    "        X_sub = fdata['arr_0']\n",
    "    \n",
    "    # entire second half of features\n",
    "    X_sub_2nd = X_sub[half_idx:]\n",
    "    \n",
    "    # scale with the same scaler\n",
    "    first_lag = lag_list[0]\n",
    "    scaler = trained_models[(levels, window, first_lag)]['scaler']\n",
    "    X_sub_2nd_scaled = scaler.transform(X_sub_2nd)\n",
    "    \n",
    "    # relevant indices for blending portion, relative to second half\n",
    "    blend_rel_start = blend_start - half_idx\n",
    "    blend_rel_end   = blend_end   - half_idx\n",
    "    X_blend_portion = X_sub_2nd_scaled[blend_rel_start:blend_rel_end]\n",
    "    \n",
    "    # gather predictions from all lags, then average\n",
    "    lag_preds_list = []\n",
    "    for lag in lag_list:\n",
    "        model_lag = trained_models[(levels, window, lag)]['model']\n",
    "        preds_blend = model_lag.predict(X_blend_portion)\n",
    "        lag_preds_list.append(preds_blend)\n",
    "    \n",
    "    avg_preds_blend = np.mean(lag_preds_list, axis=0)  # shape (#blend_samples,)\n",
    "    blend_subset_preds.append(avg_preds_blend)\n",
    "\n",
    "# shape (#blend_samples, #subsets)\n",
    "X_blend_input = np.column_stack(blend_subset_preds)\n",
    "print(f\"  >> Blending input shape = {X_blend_input.shape}\")\n",
    "\n",
    "blender = ElasticNet(alpha=1.0, l1_ratio=0.5, positive=True)\n",
    "blender.fit(X_blend_input, y_blend)\n",
    "print(\"  >> [BLENDER] ElasticNet training complete.\")\n",
    "\n",
    "###############################################################################\n",
    "# STAGE 7: EVALUATE THE BLENDED MODEL ON THE TEST PORTION\n",
    "###############################################################################\n",
    "print(\"\\n=== STAGE 7: EVALUATE BLENDED MODEL ON THE TEST PORTION ===\")\n",
    "\n",
    "test_subset_preds = []\n",
    "for (levels, window) in subset_keys:\n",
    "    feat_path = f'./data/signature_features_depth_2_levels_{levels}_window_{window}.npz'\n",
    "    with np.load(feat_path) as fdata:\n",
    "        X_sub = fdata['arr_0']\n",
    "    \n",
    "    X_sub_2nd = X_sub[half_idx:]\n",
    "    scaler = trained_models[(levels, window, lag_list[0])]['scaler']\n",
    "    X_sub_2nd_scaled = scaler.transform(X_sub_2nd)\n",
    "    \n",
    "    # slice test portion in second half\n",
    "    test_rel_start = test_start - half_idx\n",
    "    test_rel_end   = test_end   - half_idx\n",
    "    X_test_portion = X_sub_2nd_scaled[test_rel_start:test_rel_end]\n",
    "    \n",
    "    # average predictions across lags\n",
    "    lag_preds_test_list = []\n",
    "    for lag in lag_list:\n",
    "        model_lag = trained_models[(levels, window, lag)]['model']\n",
    "        preds_test = model_lag.predict(X_test_portion)\n",
    "        lag_preds_test_list.append(preds_test)\n",
    "    \n",
    "    avg_preds_test = np.mean(lag_preds_test_list, axis=0)\n",
    "    test_subset_preds.append(avg_preds_test)\n",
    "\n",
    "X_test_input = np.column_stack(test_subset_preds)  # shape (#test_samples, #subsets)\n",
    "y_pred_test_blend = blender.predict(X_test_input)\n",
    "\n",
    "test_r2 = r2_score(y_test, y_pred_test_blend)\n",
    "print(f\"  >> [RESULT] Final Blended R^2 on Test portion (y_87) = {test_r2:.4f}\")\n",
    "print(\"\\nPipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xty_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
