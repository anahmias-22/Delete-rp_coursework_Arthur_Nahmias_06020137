{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "from models.model import Informer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ParamSpace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# STAGE 1: DEFINITIONS AND HYPERPARAMETER SPACE\n",
    "###############################################################################\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Hyperparameter Search Space\n",
    "def get_hyperparameter_space(trial):\n",
    "    return {\n",
    "        'd_model': trial.suggest_categorical('d_model', [128, 256, 512]),\n",
    "        'n_heads': trial.suggest_categorical('n_heads', [2, 4, 8]),\n",
    "        'e_layers': trial.suggest_int('e_layers', 1, 3),\n",
    "        'd_layers': trial.suggest_int('d_layers', 1, 2),\n",
    "        'factor': trial.suggest_categorical('factor', [3, 5, 7]),\n",
    "        \n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-2),\n",
    "        'dropout': trial.suggest_uniform('dropout', 0.05, 0.5),\n",
    "        'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n",
    "        \n",
    "        'seq_len': trial.suggest_categorical('seq_len', [48, 96, 168]),\n",
    "        'label_len': trial.suggest_categorical('label_len', [1,24, 48]),\n",
    "        'pred_len': trial.suggest_categorical('pred_len', [1])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Fine_tune function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models.model import Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# STAGE 2: Fine-Tuning Function \n",
    "###############################################################################\n",
    "def objective(trial, train_data, val_data):\n",
    "    # Unpack the training and validation data\n",
    "    (X_train, temporal_train, y_train), (X_val, temporal_val, y_val) = train_data, val_data\n",
    "\n",
    "    params = get_hyperparameter_space(trial)\n",
    "\n",
    "    # Model Initialization\n",
    "    model = Informer(\n",
    "        enc_in=X_train.shape[2],  \n",
    "        dec_in=X_train.shape[2],\n",
    "        c_out=1,\n",
    "        seq_len=params['seq_len'],\n",
    "        label_len=params['label_len'],\n",
    "        out_len=params['pred_len'],\n",
    "        factor=params['factor'],\n",
    "        d_model=params['d_model'],\n",
    "        n_heads=params['n_heads'],\n",
    "        e_layers=params['e_layers'],\n",
    "        d_layers=params['d_layers'],\n",
    "        dropout=params['dropout'],\n",
    "        distil=True\n",
    "    )\n",
    "\n",
    "    # Loss & Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    batch_size = params['batch_size']\n",
    "    num_epochs = 5  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        permutation = torch.randperm(X_train.size(0))\n",
    "\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            indices = permutation[i:i + batch_size]\n",
    "            batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "            batch_x_mark = temporal_train[indices]  # Temporal features\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x, batch_x_mark, batch_x, batch_x_mark)  # Forward pass\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation on Validation Set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val, temporal_val, X_val, temporal_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "\n",
    "    return val_loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# STAGE 3: Data Pre-Processing \n",
    "###############################################################################\n",
    "\n",
    "# Load the dataset\n",
    "final_df = pd.read_csv(\"/Users/arthur/Documents/STUDY/Imperial/rough paths /salvi notebook/Sig_global_local_level_by_level_depth_{max_depths}.csv\")\n",
    "\n",
    "# Split features and target\n",
    "X = final_df.drop(\"y\", axis=1).values\n",
    "y = final_df[\"y\"].values\n",
    "\n",
    "# Extract timestamps (assuming first column contains timestamps)\n",
    "timestamps = X[:, 0]\n",
    "features = X[:, 1:]  # Remaining features\n",
    "\n",
    "# Standardize the features (excluding timestamps)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Reshape for Informer input (similar to LSTM)\n",
    "X_scaled = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "timestamps_dt = pd.to_datetime(pd.Series(timestamps), unit='s', errors='coerce')\n",
    "\n",
    "# Extract temporal features and clip to valid ranges\n",
    "temporal_features = pd.DataFrame({\n",
    "    'month': timestamps_dt.dt.month.fillna(1) - 1,          # 0-11 (shift from 1-12)\n",
    "    'day_of_month': timestamps_dt.dt.day.fillna(1) - 1,     # 0-30 (shift from 1-31)\n",
    "    'day_of_week': timestamps_dt.dt.dayofweek.fillna(0),    # 0-6 (already fine)\n",
    "    'hour': timestamps_dt.dt.hour.fillna(0),                # 0-23 (already fine)\n",
    "    'minute': (timestamps_dt.dt.minute.fillna(0) // 10)     # 0-5 (binning into 6 categories)\n",
    "})\n",
    "\n",
    "# Ensure the values are within the correct bounds to prevent IndexErrors\n",
    "temporal_features['month'] = temporal_features['month'].clip(0, 11)\n",
    "temporal_features['day_of_month'] = temporal_features['day_of_month'].clip(0, 30)\n",
    "temporal_features['day_of_week'] = temporal_features['day_of_week'].clip(0, 6)\n",
    "temporal_features['hour'] = temporal_features['hour'].clip(0, 23)\n",
    "temporal_features['minute'] = temporal_features['minute'].clip(0, 5)\n",
    "\n",
    "# Reshape temporal features to match the sequence shape\n",
    "temporal_features = temporal_features.values.reshape((X_scaled.shape[0], 1, -1))\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, temporal_train, temporal_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, temporal_features, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# Convert temporal features to 'long' for embedding compatibility\n",
    "temporal_train = torch.tensor(temporal_train, dtype=torch.long)\n",
    "temporal_val = torch.tensor(temporal_val, dtype=torch.long)\n",
    "\n",
    "# Target variables\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Features Shape: (100, 1, 5)\n",
      "X_train Shape: torch.Size([80, 1260, 1])\n",
      "Temporal_train Shape: torch.Size([80, 1, 5])\n",
      "y_train Shape: torch.Size([80, 1])\n",
      "NaNs in Temporal Features: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Temporal Features Shape:\", temporal_features.shape)\n",
    "print(\"X_train Shape:\", X_train.shape)\n",
    "print(\"Temporal_train Shape:\", temporal_train.shape)\n",
    "print(\"y_train Shape:\", y_train.shape)\n",
    "print(\"NaNs in Temporal Features:\", np.isnan(temporal_features).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   1970-01-01 01:55:49\n",
      "1   1970-01-01 02:20:47\n",
      "2   1970-01-01 02:53:14\n",
      "3   1970-01-01 04:04:52\n",
      "4   1970-01-01 04:24:29\n",
      "dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(timestamps_dt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample X: torch.Size([1260, 1])\n",
      "Sample Temporal Features: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Check compatibility\n",
    "print(\"Sample X:\", X_train[0].shape)         # Should be (1260, 1)\n",
    "print(\"Sample Temporal Features:\", temporal_train[0].shape)  # Should be (1, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters search using Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-02-03 19:38:27,734]\u001b[0m A new study created in memory with name: no-name-aff11cad-8ea7-4c0e-bbb4-a2eb450ca03a\u001b[0m\n",
      "/Users/arthur/opt/anaconda3/envs/informer_env/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "/Users/arthur/opt/anaconda3/envs/informer_env/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "/Users/arthur/opt/anaconda3/envs/informer_env/lib/python3.6/site-packages/ipykernel_launcher.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "/Users/arthur/opt/anaconda3/envs/informer_env/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/arthur/opt/anaconda3/envs/informer_env/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[32m[I 2025-02-03 19:43:49,998]\u001b[0m Trial 0 finished with value: 0.3632199764251709 and parameters: {'d_model': 256, 'n_heads': 2, 'e_layers': 3, 'd_layers': 1, 'factor': 7, 'batch_size': 16, 'learning_rate': 0.0017840984509659957, 'dropout': 0.39543284486153923, 'weight_decay': 2.0980701910994343e-06, 'seq_len': 48, 'label_len': 1, 'pred_len': 1}. Best is trial 0 with value: 0.3632199764251709.\u001b[0m\n",
      "\u001b[32m[I 2025-02-03 19:52:11,747]\u001b[0m Trial 1 finished with value: 0.29379910230636597 and parameters: {'d_model': 256, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'factor': 3, 'batch_size': 16, 'learning_rate': 0.0007380398380125416, 'dropout': 0.2044713871292106, 'weight_decay': 2.0278925123447975e-06, 'seq_len': 96, 'label_len': 24, 'pred_len': 1}. Best is trial 1 with value: 0.29379910230636597.\u001b[0m\n",
      "/Users/arthur/opt/anaconda3/envs/informer_env/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[32m[I 2025-02-03 20:00:08,048]\u001b[0m Trial 2 finished with value: 0.3614947199821472 and parameters: {'d_model': 128, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'factor': 3, 'batch_size': 64, 'learning_rate': 0.006438969181281324, 'dropout': 0.18127684800554456, 'weight_decay': 0.0008726016996075757, 'seq_len': 168, 'label_len': 48, 'pred_len': 1}. Best is trial 1 with value: 0.29379910230636597.\u001b[0m\n",
      "/Users/arthur/opt/anaconda3/envs/informer_env/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[32m[I 2025-02-03 20:04:41,267]\u001b[0m Trial 3 finished with value: 0.27522388100624084 and parameters: {'d_model': 256, 'n_heads': 8, 'e_layers': 3, 'd_layers': 2, 'factor': 3, 'batch_size': 32, 'learning_rate': 0.0010603708030599993, 'dropout': 0.33546555311500165, 'weight_decay': 1.5037746319349247e-05, 'seq_len': 48, 'label_len': 1, 'pred_len': 1}. Best is trial 3 with value: 0.27522388100624084.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# STAGE 4: Hyperparameters Search\n",
    "###############################################################################\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "pruner = MedianPruner()\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, (X_train, temporal_train, y_train), (X_val, temporal_val, y_val)),\n",
    "    n_trials=50\n",
    ")\n",
    "\n",
    "# Display Best Hyperparameters\n",
    "print(\"Best Trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"Validation Loss: {trial.value}\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# STAGE 5: TRAINING USING BEST HYPERPARAMETERS\n",
    "###############################################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models.model import Informer\n",
    "\n",
    "# Training Function\n",
    "def train_model(best_params, X_train, y_train, X_val, y_val, num_epochs=50):\n",
    "    # Initialize model with best hyperparameters\n",
    "    model = Informer(\n",
    "        enc_in=X_train.shape[2],\n",
    "        dec_in=X_train.shape[2],\n",
    "        c_out=1,\n",
    "        seq_len=best_params['seq_len'],\n",
    "        label_len=best_params['label_len'],\n",
    "        out_len=best_params['pred_len'],\n",
    "        factor=best_params['factor'],\n",
    "        d_model=best_params['d_model'],\n",
    "        n_heads=best_params['n_heads'],\n",
    "        e_layers=best_params['e_layers'],\n",
    "        d_layers=best_params['d_layers'],\n",
    "        dropout=best_params['dropout'],\n",
    "        distil=True\n",
    "    )\n",
    "\n",
    "    # Loss & Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "\n",
    "    batch_size = best_params['batch_size']\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(X_train.size(0))\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            indices = permutation[i:i + batch_size]\n",
    "            batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "            batch_x_mark = temporal_train[indices]  # Add temporal data for training\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x, batch_x_mark, batch_x, batch_x_mark)  # Pass temporal features\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation Loss Calculation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val, X_val, None)\n",
    "            val_loss = criterion(val_output, y_val)\n",
    "\n",
    "        train_losses.append(epoch_loss / (X_train.size(0) // batch_size))\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# STAGE 6: EVALUATE THE MODEL \n",
    "###############################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_val, X_val, None)\n",
    "    \n",
    "    predictions = predictions.squeeze().cpu().numpy()\n",
    "    y_true = y_val.squeeze().cpu().numpy()\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    mse = mean_squared_error(y_true, predictions)\n",
    "    mae = mean_absolute_error(y_true, predictions)\n",
    "    r2 = r2_score(y_true, predictions)\n",
    "\n",
    "    print(f\"Evaluation Metrics:\")\n",
    "    print(f\" - MSE: {mse:.4f}\")\n",
    "    print(f\" - MAE: {mae:.4f}\")\n",
    "    print(f\" - R² Score: {r2:.4f}\")\n",
    "\n",
    "    # Plotting Predictions vs Actual Values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_true[:100], label='Actual')\n",
    "    plt.plot(predictions[:100], label='Predicted')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Target Value')\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return mse, mae, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# STAGE 7: MAIN\n",
    "###############################################################################\n",
    "\n",
    "# Best hyperparameters from Optuna\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Train the model\n",
    "model, train_losses, val_losses = train_model(best_params, X_train, y_train, X_val, y_val, num_epochs=50)\n",
    "\n",
    "# We now Evaluate the model \n",
    "mse, mae, r2 = evaluate_model(model, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "informer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
