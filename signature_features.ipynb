{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import iisignature\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the transform function from augment do several good transformation for the data set, it adds a bespoint, it normalizes the data, it add a time feature ( i.e. use timestep = {1,2,3..} to make time =[0.00, 0.33, 0.66, ...]), it finally lags the features ( move L step forward). \n",
    "\n",
    "Then the transform method from the class fetaure extraction compute the signautre, first it initialize a depth for the signature, then it encapsulates the data set into levels ( levels of the order book , each level has 4 features bid ask prices and bid asd volums) and for each subset that correspond to a level we add the midprice columns ( the last columns). Se for each level we have a dataset of 5 columns. Then we apply the augment method defined previously and we finally compute the signature using the signature library. The size of X_out is then the number of levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augment(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, normalize=False, add_time=True, lead_lag=False, basepoint=True, t0=0.0, t1=1.0, lags=[1]):\n",
    "        self.normalize = normalize\n",
    "        self.add_time = add_time\n",
    "        self.lead_lag = lead_lag\n",
    "        self.basepoint = basepoint\n",
    "        self.t0, self.t1 = t0, t1\n",
    "        self.lags = lags\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "  \n",
    "    def transform(self, X):\n",
    "        if self.normalize:\n",
    "            X /= X.max(axis=1)[:, None, :]\n",
    "        if self.lead_lag:\n",
    "            X_list = [X] \n",
    "            for lag in self.lags:\n",
    "                X_shifted = np.zeros_like(X)\n",
    "                X_shifted[:, lag:, :] = X[:, :-lag, :]                \n",
    "                X_shifted[:, :lag, :] = np.expand_dims(X[:, 0, :], axis=1)\n",
    "                X_list.append(X_shifted)\n",
    "            X = np.concatenate(X_list, axis=-1)\n",
    "        if self.add_time:\n",
    "            time = np.linspace(self.t0, self.t1, X.shape[1])\n",
    "            X = np.concatenate((np.tile(time[None, :, None], [X.shape[0], 1, 1]), X), axis=-1)\n",
    "        if self.basepoint:\n",
    "            X = np.concatenate((np.zeros_like(X[:, :1]), X), axis=1)\n",
    "        return X\n",
    "    \n",
    "class FeatureExtraction(Augment):\n",
    "\n",
    "    def __init__(self, \n",
    "                 depth=2, \n",
    "                 t0=0.0, \n",
    "                 t1=1.0, \n",
    "                 lags=[1], \n",
    "                 levels=1,\n",
    "                 level_by_level=True, \n",
    "                 normalize=False, \n",
    "                 add_time=True, \n",
    "                 lead_lag=False, \n",
    "                 basepoint=False\n",
    "                 ):\n",
    "        \n",
    "        Augment.__init__(self, normalize, add_time, lead_lag, basepoint, t0, t1, lags)\n",
    "        self.depth = depth\n",
    "        self.levels = levels\n",
    "        self.level_by_level = level_by_level\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_sig_list = []\n",
    "        indices = np.arange(self.levels) if self.level_by_level else [None]\n",
    "        for j in indices: \n",
    "            if j is not None:\n",
    "                start_idx = 4 * j # for each level we capture 4 columns , levels are the levels of the order book)\n",
    "                end_idx = 4 * (j + 1)\n",
    "            else:\n",
    "                start_idx = 0\n",
    "                end_idx = 4 * self.levels\n",
    "            X_window = X[:, :, start_idx:end_idx]\n",
    "            X_midprice = X[:, :, -1:]\n",
    "            X_window = np.concatenate([X_window, X_midprice], axis=2)\n",
    "            X_augment = Augment.transform(self, X_window)\n",
    "            X_sig = iisignature.sig(X_augment, self.depth)\n",
    "            X_sig_list.append(X_sig)\n",
    "        X_out = np.concatenate(X_sig_list, axis=1)\n",
    "        return X_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askRate0\n",
      "askRate1\n",
      "askRate2\n",
      "askRate3\n",
      "askRate4\n",
      "askRate5\n",
      "askRate6\n",
      "askRate7\n",
      "askRate8\n",
      "askRate9\n",
      "askRate10\n",
      "askRate11\n",
      "askRate12\n",
      "askRate13\n",
      "askRate14\n",
      "askSize0\n",
      "askSize1\n",
      "askSize2\n",
      "askSize3\n",
      "askSize4\n",
      "askSize5\n",
      "askSize6\n",
      "askSize7\n",
      "askSize8\n",
      "askSize9\n",
      "askSize10\n",
      "askSize11\n",
      "askSize12\n",
      "askSize13\n",
      "askSize14\n",
      "bidRate0\n",
      "bidRate1\n",
      "bidRate2\n",
      "bidRate3\n",
      "bidRate4\n",
      "bidRate5\n",
      "bidRate6\n",
      "bidRate7\n",
      "bidRate8\n",
      "bidRate9\n",
      "bidRate10\n",
      "bidRate11\n",
      "bidRate12\n",
      "bidRate13\n",
      "bidRate14\n",
      "bidSize0\n",
      "bidSize1\n",
      "bidSize2\n",
      "bidSize3\n",
      "bidSize4\n",
      "bidSize5\n",
      "bidSize6\n",
      "bidSize7\n",
      "bidSize8\n",
      "bidSize9\n",
      "bidSize10\n",
      "bidSize11\n",
      "bidSize12\n",
      "bidSize13\n",
      "bidSize14\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "nrows = 500000\n",
    "data = pd.read_csv('/Users/arthur/Documents/STUDY/Imperial/rough paths /salvi notebook/data.csv.gz', compression='gzip', nrows=nrows)\n",
    "\n",
    "for col in data.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askRate0\n",
      "bidRate0\n",
      "askSize0\n",
      "bidSize0\n",
      "askRate1\n",
      "bidRate1\n",
      "askSize1\n",
      "bidSize1\n",
      "askRate2\n",
      "bidRate2\n",
      "askSize2\n",
      "bidSize2\n",
      "askRate3\n",
      "bidRate3\n",
      "askSize3\n",
      "bidSize3\n",
      "askRate4\n",
      "bidRate4\n",
      "askSize4\n",
      "bidSize4\n",
      "askRate5\n",
      "bidRate5\n",
      "askSize5\n",
      "bidSize5\n",
      "askRate6\n",
      "bidRate6\n",
      "askSize6\n",
      "bidSize6\n",
      "askRate7\n",
      "bidRate7\n",
      "askSize7\n",
      "bidSize7\n",
      "askRate8\n",
      "bidRate8\n",
      "askSize8\n",
      "bidSize8\n",
      "askRate9\n",
      "bidRate9\n",
      "askSize9\n",
      "bidSize9\n",
      "askRate10\n",
      "bidRate10\n",
      "askSize10\n",
      "bidSize10\n",
      "askRate11\n",
      "bidRate11\n",
      "askSize11\n",
      "bidSize11\n",
      "askRate12\n",
      "bidRate12\n",
      "askSize12\n",
      "bidSize12\n",
      "askRate13\n",
      "bidRate13\n",
      "askSize13\n",
      "bidSize13\n",
      "askRate14\n",
      "bidRate14\n",
      "askSize14\n",
      "bidSize14\n",
      "midprice\n"
     ]
    }
   ],
   "source": [
    "nrows = 500000\n",
    "\n",
    "data = pd.read_csv('/Users/arthur/Documents/STUDY/Imperial/rough paths /salvi notebook/data.csv.gz', compression='gzip', nrows=nrows)\n",
    "\n",
    "# set the NaN sizes sizes to zero\n",
    "askSize_columns = [f'askSize{i}' for i in range(15)] \n",
    "bidSize_columns = [f'bidSize{i}' for i in range(15)]\n",
    "size_columns = askSize_columns + bidSize_columns\n",
    "data[size_columns] = data[size_columns].fillna(0.0)\n",
    "# set the NaN rates to last observed value and fill the remaining ones backward\n",
    "askRate_columns = [f'askRate{i}' for i in range(15)] \n",
    "bidRate_columns = [f'bidRate{i}' for i in range(15)]\n",
    "rate_columns = askRate_columns + bidRate_columns\n",
    "data[rate_columns] = data[rate_columns].ffill().bfill()\n",
    "data['midprice'] = 0.5 * (data['askRate0'] + data['bidRate0']) \n",
    "\n",
    "# reorder columns\n",
    "selected_columns = [[f'askRate{i}', f'bidRate{i}', f'askSize{i}', f'bidSize{i}'] for i in range(15)]\n",
    "selected_columns = list(itertools.chain.from_iterable(selected_columns)) + ['midprice']\n",
    "data = data[selected_columns]\n",
    "for col in data.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestep</th>\n",
       "      <th>target_lag_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestep  target_lag_100\n",
       "0         0           -0.50\n",
       "1         1           -0.50\n",
       "2         2           -0.50\n",
       "3         3           -0.25\n",
       "4         4           -0.25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Lag values\n",
    "lags = [57, 65, 75, 80, 87, 100]\n",
    "# Compute target for each lag\n",
    "for lag in lags:\n",
    "    target_data = pd.DataFrame({\n",
    "        'timestep': data.index[:-lag],\n",
    "        f'target_lag_{lag}': data['midprice'].shift(-lag)[:-lag].values - data['midprice'][:-lag].values\n",
    "    })\n",
    "    \n",
    "    # Save the target data to CSV\n",
    "    filename = f'lag_{lag}_target.csv'\n",
    "    target_data.to_csv(filename, index=False)\n",
    "\n",
    "target_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the window transform create a matrix of size (n, d, M ) with n the number of time step, d the number of features and M the size of the widow. the temp object contains all the different window sliding into the time step ( if n =10, M=5 : 6 windows) and each window has d features. The X_windows still have n rows but the the M-1 (  size of the windows) rows are empty because there is no espace to have a window, then each row is equal to a window containing the M previous values. The leave out start and leave out end cut the end and the beginning of the X_window to have only the core information, it is set to 500 as the max window size is 200. \n",
    "\n",
    "Next for each depth ( S(k)), \n",
    "    for each level (level of order book): \n",
    "        we create an object of feature extraction that contains the method signature, chunksize is the number of parallel computation\n",
    "for the dimension we will have data of 6 features : 4 features per level ( bid/ask/price/volume)\n",
    "the midprice and the basepoint , and the signature are added one to eachother : 90 = 6*15; 630 = 6*6*15+90 , 3870 = 6*6*6*15+630,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499000, 90)\n",
      "Processed window 50, level 15, depth 1, shape: (499000, 90)\n",
      "Saved: dataset_window-50_level-15_depth-1.csv\n",
      "(499000, 630)\n",
      "Processed window 50, level 15, depth 2, shape: (499000, 630)\n",
      "Saved: dataset_window-50_level-15_depth-2.csv\n",
      "(499000, 3870)\n",
      "Processed window 50, level 15, depth 3, shape: (499000, 3870)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [56:52<56:52, 3412.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: dataset_window-50_level-15_depth-3.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "leave_out_start = 500\n",
    "leave_out_end = 500\n",
    "\n",
    "M, N = data.shape\n",
    "\n",
    "# extract signature features\n",
    "for window in tqdm([ 50, 87]):\n",
    "    # segment data\n",
    "    temp = np.lib.stride_tricks.sliding_window_view(data, window_shape=window, axis=0).transpose(0, 2, 1)\n",
    "    X_windows = np.full((M, window, N), np.nan)\n",
    "    X_windows[window - 1:, ...] = temp\n",
    "    X_windows = X_windows[leave_out_start:-leave_out_end, ...]\n",
    "    for depth in [1, 2,3]:\n",
    "        for levels in [ 15]:\n",
    "            feature_extraction = FeatureExtraction(depth=depth, levels=levels)\n",
    "            chunk_size = 1000  \n",
    "            num_chunks = (X_windows.shape[0] + chunk_size - 1) // chunk_size \n",
    "            sub_arrays = np.array_split(X_windows, num_chunks, axis=0)\n",
    "            all_transformed_chunks = []\n",
    "            for chunk in sub_arrays:\n",
    "                X_transformed_chunk = feature_extraction.fit_transform(chunk)\n",
    "                all_transformed_chunks.append(X_transformed_chunk)\n",
    "            X = np.concatenate(all_transformed_chunks, axis=0)\n",
    "            assert not np.isnan(X).any(), \"NaNs values found in signature features\"\n",
    "            print(X.shape)\n",
    "            print(f\"Processed window {window}, level {levels}, depth {depth}, shape: {X.shape}\")\n",
    "\n",
    "            # Convert to Pandas DataFrame\n",
    "            df = pd.DataFrame(X)\n",
    "\n",
    "            # Define filename with format: dataset_window-{window}_level-{level}_depth-{depth}.csv\n",
    "            filename = f\"dataset_window-{window}_level-{levels}_depth-{depth}.csv\"\n",
    "\n",
    "            # Save dataset as CSV\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will rename the columns of the signatures so that we can understand it easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dont forget to suffle the data set for the entry \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "leave_out_start = 500\n",
    "leave_out_end = 500\n",
    "nrows = 500000\n",
    "\n",
    "M, N = data.shape  # Get dataset shape\n",
    "window_sizes = [50, 87]  # Define window sizes\n",
    "levels = 15  # Number of order book levels\n",
    "depths = [1, 2, 3]  # Signature depths\n",
    "num_features_per_level = 6  # **6 features per level (5 original + basepoint)**\n",
    "\n",
    "# Initialize storage for the final dataset\n",
    "final_dataset = []\n",
    "\n",
    "for window in tqdm(window_sizes):\n",
    "    # Step 1: Segment the data into rolling windows\n",
    "    temp = np.lib.stride_tricks.sliding_window_view(data, window_shape=window, axis=0).transpose(0, 2, 1)\n",
    "    X_windows = np.full((M, window, N), np.nan)\n",
    "    X_windows[window - 1:, ...] = temp\n",
    "    X_windows = X_windows[leave_out_start:-leave_out_end, ...]  # Trim unwanted rows\n",
    "\n",
    "    # Extract last timestamp for each window (most recent time in window)\n",
    "    time_column = data.index[leave_out_start + window - 1:-leave_out_end].values\n",
    "\n",
    "    # Initialize empty list to store all levels' signature features\n",
    "    signature_features = []\n",
    "    column_names = [\"time\"]  # First column is time\n",
    "\n",
    "    for depth in depths:\n",
    "        feature_extraction = FeatureExtraction(depth=depth, levels=levels)\n",
    "        \n",
    "        # Chunking mechanism to avoid memory overflow\n",
    "        chunk_size = 1000  \n",
    "        num_chunks = (X_windows.shape[0] + chunk_size - 1) // chunk_size\n",
    "        sub_arrays = np.array_split(X_windows, num_chunks, axis=0)\n",
    "\n",
    "        all_transformed_chunks = []\n",
    "\n",
    "        # Process each chunk and extract features\n",
    "        for chunk in sub_arrays:\n",
    "            X_transformed_chunk = feature_extraction.fit_transform(chunk)\n",
    "            all_transformed_chunks.append(X_transformed_chunk)\n",
    "\n",
    "        # Concatenate transformed data across chunks\n",
    "        X = np.concatenate(all_transformed_chunks, axis=0)\n",
    "\n",
    "        # Ensure no NaN values\n",
    "        assert not np.isnan(X).any(), \"NaNs values found in signature features\"\n",
    "        print(f\"Processed window {window}, level {levels}, depth {depth}, shape: {X.shape}\")\n",
    "\n",
    "        # Append signature features for each depth\n",
    "        signature_features.append(X)\n",
    "\n",
    "        # Step 4: Generate column names dynamically\n",
    "        for level in range(1, levels + 1):\n",
    "            if depth == 1:  # Depth 1 (Vector)\n",
    "                for f1 in range(num_features_per_level):\n",
    "                    column_names.append(f\"level_{level}_sig_depth_1_feature_{f1}\")\n",
    "\n",
    "            elif depth == 2:  # Depth 2 (Matrix)\n",
    "                for f1, f2 in itertools.product(range(num_features_per_level), repeat=2):\n",
    "                    column_names.append(f\"level_{level}_sig_depth_2_feature_{f1}_feature_{f2}\")\n",
    "\n",
    "            elif depth == 3:  # Depth 3 (Tensor)\n",
    "                for f1, f2, f3 in itertools.product(range(num_features_per_level), repeat=3):\n",
    "                    column_names.append(f\"level_{level}_sig_depth_3_feature_{f1}_feature_{f2}_feature_{f3}\")\n",
    "\n",
    "    # Concatenate all computed signatures for Depth 1, 2, 3\n",
    "    X_final = np.concatenate(signature_features, axis=1)\n",
    "   \n",
    "    data = pd.read_csv('/Users/arthur/Documents/STUDY/Imperial/rough paths /salvi notebook/data.csv.gz', compression='gzip', nrows=nrows)\n",
    "\n",
    "    # Extract y target values (corresponding to the last row in each window)\n",
    "    y_values = data.iloc[leave_out_start + window - 1:-leave_out_end, -1].values  # Assuming y is the last column\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(X_final, columns=column_names[1:])  # Skip \"time\" for now\n",
    "\n",
    "    # Add time and target column to DataFrame\n",
    "    df.insert(0, \"time\", time_column)\n",
    "    df[\"y\"] = y_values\n",
    "\n",
    "    # Save dataset\n",
    "    filename = f\"final_dataset_window-{window}_levels-{levels}_depths-{len(depths)}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "    # Append to final dataset list\n",
    "    final_dataset.append(df)\n",
    "\n",
    "# Concatenate final dataset for all window sizes if needed\n",
    "final_df = pd.concat(final_dataset, axis=0)\n",
    "final_df.to_csv(\"complete_signature_dataset.csv\", index=False)\n",
    "print(\"Final dataset saved as complete_signature_dataset.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
